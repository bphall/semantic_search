{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import twint\n",
    "import json\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools FOR TOPIC MODELING\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional  (I HAVEN'T USED THIS YET)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "\n",
    "# NLTK Stop words -------------- ADD TO THESE AS WE SEE FIT\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_page = requests.get('https://www.gutenberg.org/browse/scores/top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(html_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100 = []\n",
    "for link in soup.find_all('a'):\n",
    "    top_100.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100 = top_100[19:119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_names = []\n",
    "for link in soup.find_all('a'):\n",
    "    top_100_names.append(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_names = top_100_names[19:119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rid_paren(book_name):\n",
    "    clean = ''.join(re.findall(r'\\b\\w+[^()\\d+]\\b', book_name))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = []\n",
    "for i in top_100_names:\n",
    "    book_names.append(rid_paren(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {}\n",
    "titles = []\n",
    "for i in book_names:\n",
    "    try:\n",
    "        name_author = i.split(' by ')\n",
    "        book_dict[name_author[0]] = name_author[1]\n",
    "        titles.append(name_author[0])\n",
    "    except:\n",
    "        book_dict[i] = i\n",
    "        titles.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indexed_titles = {}\n",
    "indexed_urls = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0,100):\n",
    "    indexed_titles[i] = titles[count]\n",
    "    indexed_urls[i] = url_nums[count]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_nums = []\n",
    "for i in top_100:\n",
    "    temp = re.findall(r'\\d+', i)\n",
    "    url_nums.append(int(temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_titles = {}\n",
    "indexed_urls = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0,100):\n",
    "    indexed_titles[i] = titles[count]\n",
    "    indexed_urls[i] = url_nums[count]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_text(num):\n",
    "    html_page = requests.get(f'https://www.gutenberg.org/files/{num}/{num}-h/{num}-h.htm')\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    paragraphs = soup.findAll('p')\n",
    "    \n",
    "    unclean_text = []\n",
    "    for i in paragraphs:\n",
    "        unclean_text.append(i.text)\n",
    "\n",
    "    unclean_text = ' '.join(unclean_text)\n",
    "\n",
    "    clean_text = re.sub('\\n', ' ', unclean_text)\n",
    "    clean_text = re.sub('\\r', ' ', clean_text)\n",
    "    clean_text = re.sub('\\xa0', '', clean_text)\n",
    "    clean_text = re.sub(' +', ' ', clean_text)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = strip_text(indexed_urls[36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in url_nums[:50]:\n",
    "    novel_dict[titles[book_index]] = strip_text(i)\n",
    "    book_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(novel_dict, open(\"pickled_novel_dict\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 69400\r\n",
      "-rw-r--r--  1 braytonhall  staff    27B Apr  5 17:12 README.md\r\n",
      "-rw-r--r--  1 braytonhall  staff   424K Apr  6 00:02 main_scratch.ipynb\r\n",
      "-rw-r--r--  1 braytonhall  staff    33M Apr  6 00:02 pickled_novel_dict\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_fifty = pickle.load( open( \"pickled_novel_dict\", \"rb\" ) )\n",
    "# favorite_color is now { \"lion\": \"yellow\", \"kitty\": \"red\" }d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Pride and Prejudice', 'FrankensteinOrThe Modern Prometheus', 'A Journal of the Plague Year', 'The Importance of Being EarnestA Trivial Comedy for Serious People', 'The Works of Edgar Allan PoeThe Raven Edition', \"Alice's Adventures in Wonderland\", 'The Call of the Wild', 'Et dukkehjemEnglish', 'A Modest Proposal', 'The Strange Case of DrJekyll and MrHyde', 'A Tale of Two Cities', 'Treasure Island', 'The Yellow Wallpaper', 'Ion', 'Adventures of Huckleberry Finn', 'The Adventures of Sherlock Holmes', 'Anthem', 'The Adventures of Tom Sawyer', 'Moby DickOrThe Whale', 'A Christmas Carol in ProseBeing a Ghost Story of Christmas', 'Little Women', 'The Masque of the Red Death', 'Metamorphosis', 'Heart of Darkness', 'GrimmsFairy Tales', 'The Picture of Dorian Gray', 'Emma', 'The Hound of the Baskervilles', 'Waldenand On The Duty Of Civil Disobedience', 'Peter Pan', 'The Decameron of Giovanni Boccaccio', 'Dracula', 'The Awakeningand Selected Short Stories', 'Jane EyreAn Autobiography', 'The Scarlet Letter', 'Great Expectations', 'War and Peace', 'The Secret Garden', 'Il PrincipeEnglish', 'The War of the Worlds', 'BeowulfAn Anglo-Saxon Epic Poem', 'The Wonderful Wizard of Oz', 'The Count of Monte CristoIllustrated', 'Anne of Green Gables', 'Ulysses', 'The MemoirsCorrespondenceAnd MiscellaniesFrom The Papers Of Thomas Jefferson', 'The Mysterious Affair at Styles', 'Oliver Twist', 'Pygmalion'])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_fifty.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68576\r\n",
      "drwxr-xr-x   7 braytonhall  staff   224B Apr  6 00:05 \u001b[34m.\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x+ 61 braytonhall  staff   1.9K Apr  5 17:12 \u001b[34m..\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x  12 braytonhall  staff   384B Apr  5 17:13 \u001b[34m.git\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x   3 braytonhall  staff    96B Apr  5 17:14 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m/\r\n",
      "-rw-r--r--   1 braytonhall  staff    27B Apr  5 17:12 README.md\r\n",
      "-rw-r--r--   1 braytonhall  staff    10K Apr  6 00:05 main_scratch.ipynb\r\n",
      "-rw-r--r--   1 braytonhall  staff    33M Apr  6 00:02 pickled_novel_dict\r\n"
     ]
    }
   ],
   "source": [
    "ls -lha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
